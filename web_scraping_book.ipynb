{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cUAhDvDKRNSI",
        "Jd7-d6eUjjwR",
        "7cn0e6ukrELq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Book - Web Scraping with Python - Collecting More Data From the Modern Web - O'Reilly"
      ],
      "metadata": {
        "id": "2VoyHj71Q8iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beautiful Soup 4 - https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ],
      "metadata": {
        "id": "3Zd5fu8hRAeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "## Part 1 - Building Scrapers\n",
        "1. Your First Web Scraper\n",
        "2. Advanced HTML Parsing\n",
        "3. Writing Web Crawlers\n",
        "4. Web Crawling Models\n",
        "5. Scrapy\n",
        "6. Storing Data\n",
        "\n",
        "## Part 2 - Advanced Scraping\n",
        "7. Reading Documents\n",
        "8. Cleaning Your Dirty Data\n",
        "9. Reading and Writing Natural Languages\n",
        "10. Crawling Throug Forms and Logins\n",
        "11. Scraping JavaScript\n",
        "12. Crawling Through APIs\n",
        "13. Image Processing and Text Recognition\n",
        "14. Avoiding Scraping Traps\n",
        "15. Testing Your Website with Scrapers\n",
        "16. Web Crawling in Parallel\n",
        "17. Scraping Remotely\n",
        "18. The Legalities and Ethics of Web Scraping\n"
      ],
      "metadata": {
        "id": "nZ2y-D4DRC8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Building Scrapers "
      ],
      "metadata": {
        "id": "ivByuB5oRKQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Your First Web Scraper"
      ],
      "metadata": {
        "id": "cUAhDvDKRNSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
        "print(html.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcTALOHIQtoe",
        "outputId": "86721d9d-40ce-4c42-d241-ccdca4f2a510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "URLLIB documentation: https://docs.python.org/3/library/urllib.html"
      ],
      "metadata": {
        "id": "afMRldwea0Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction to Beautiful Soup"
      ],
      "metadata": {
        "id": "2E3xzPHUbGIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65OHBCiBaAad",
        "outputId": "d105029b-3c56-4fbf-8923-fced279bfef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
        "bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "print(bs.h1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs9iUaQ9byl6",
        "outputId": "18180620-aa7f-4cce-ceca-2733d4d090e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h1>An Interesting Title</h1>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WEgjimIcMhq",
        "outputId": "33a47d6f-bdd2-4a19-b0b8-48cfa7b526c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = BeautifulSoup(html.read(), 'html5lib')"
      ],
      "metadata": {
        "id": "sU1EXOc7dNHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6NbfDCsdfRL",
        "outputId": "239db829-1ebb-42c5-f85b-95ca741ee026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<html><head></head><body></body></html>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError"
      ],
      "metadata": {
        "id": "kMBGNMUtd0pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    html = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
        "except HTTPError as e:\n",
        "    print(e)\n",
        "except URLError as e:\n",
        "    print('The server could not be found!')\n",
        "else:\n",
        "    print('It Worked!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWZXmWvweV1O",
        "outputId": "2fffecf7-e7e8-448b-f658-e3868a1c8677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The server could not be found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    badContent = bs.nonExistingTag.anotherTag\n",
        "except AttributeError as e:\n",
        "    print('Tag was not found')\n",
        "else:\n",
        "    if badContent == None:\n",
        "        print ('Tag was not found')\n",
        "    else:\n",
        "        print(badContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riUobgsthwvO",
        "outputId": "3f9ae40b-7d56-4ec2-9c3b-8170256a0b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag was not found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/element.py:1110: UserWarning: .nonExistingTag is deprecated, use .find(\"nonExisting\") instead. If you really were looking for a tag called nonExistingTag, use .find(\"nonExistingTag\")\n",
            "  name=tag_name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def getTitle(url):\n",
        "    try:\n",
        "        html = urlopen(url)\n",
        "    except HTTPError as e:\n",
        "        return None\n",
        "    try:\n",
        "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "        title = bs.body.h1\n",
        "    except AttributeError as e:\n",
        "        return None\n",
        "    return title\n",
        "\n",
        "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
        "\n",
        "if title == None:\n",
        "    print('Title could not be found')\n",
        "else:\n",
        "    print(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfn0HnWWiN4p",
        "outputId": "cb37d3d9-e05d-4d2f-96bf-8e5e654d9873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h1>An Interesting Title</h1>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Advanced HTML Parsing"
      ],
      "metadata": {
        "id": "Jd7-d6eUjjwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs.find_all('table')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGGUOoQBju9P",
        "outputId": "f9157c11-3194-4e22-d59e-dbe0ae505db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
        "bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "\n",
        "nameList = bs.findAll('span', {'class':'green'})\n",
        "for name in nameList:\n",
        "    print(name.get_text())"
      ],
      "metadata": {
        "id": "jh5qqdWvlJZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nameList = bs.find_all(text='the prince')\n",
        "print(len(nameList))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP0fQNaupmY2",
        "outputId": "a5420876-5fe2-49fc-b63f-7645cfb946be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " bs.find_all(class_='green')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIsDCmOhsMHZ",
        "outputId": "095cf1ae-22a5-4561-d9dc-570181319614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<span class=\"green\">Anna\n",
              " Pavlovna Scherer</span>, <span class=\"green\">Empress Marya\n",
              " Fedorovna</span>, <span class=\"green\">Prince Vasili Kuragin</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">St. Petersburg</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the prince</span>, <span class=\"green\">the prince</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Prince Vasili</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Wintzingerode</span>, <span class=\"green\">King of Prussia</span>, <span class=\"green\">le Vicomte de Mortemart</span>, <span class=\"green\">Montmorencys</span>, <span class=\"green\">Rohans</span>, <span class=\"green\">Abbe Morio</span>, <span class=\"green\">the Emperor</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Prince Vasili</span>, <span class=\"green\">Dowager Empress Marya Fedorovna</span>, <span class=\"green\">the baron</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">Anna Pavlovna's</span>, <span class=\"green\">Her Majesty</span>, <span class=\"green\">Baron\n",
              " Funke</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anna\n",
              " Pavlovna</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anatole</span>, <span class=\"green\">the prince</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anna\n",
              " Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs.find_all('', {'class':'green'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbbXq9wVszSe",
        "outputId": "8e2810cb-b73d-4847-e35d-a7b9c785f554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<span class=\"green\">Anna\n",
              " Pavlovna Scherer</span>, <span class=\"green\">Empress Marya\n",
              " Fedorovna</span>, <span class=\"green\">Prince Vasili Kuragin</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">St. Petersburg</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the prince</span>, <span class=\"green\">the prince</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Prince Vasili</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Wintzingerode</span>, <span class=\"green\">King of Prussia</span>, <span class=\"green\">le Vicomte de Mortemart</span>, <span class=\"green\">Montmorencys</span>, <span class=\"green\">Rohans</span>, <span class=\"green\">Abbe Morio</span>, <span class=\"green\">the Emperor</span>, <span class=\"green\">the prince</span>, <span class=\"green\">Prince Vasili</span>, <span class=\"green\">Dowager Empress Marya Fedorovna</span>, <span class=\"green\">the baron</span>, <span class=\"green\">Anna Pavlovna</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">Anna Pavlovna's</span>, <span class=\"green\">Her Majesty</span>, <span class=\"green\">Baron\n",
              " Funke</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anna\n",
              " Pavlovna</span>, <span class=\"green\">the Empress</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anatole</span>, <span class=\"green\">the prince</span>, <span class=\"green\">The prince</span>, <span class=\"green\">Anna\n",
              " Pavlovna</span>, <span class=\"green\">Anna Pavlovna</span>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen('https://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "for child in bs.find('table',{'id':'giftList'}).children:\n",
        "    print(child)"
      ],
      "metadata": {
        "id": "Sf2fytfHs0bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
        "    print(sibling)"
      ],
      "metadata": {
        "id": "vR9l81DR0JyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs.find('table', {'id':'giftList'})"
      ],
      "metadata": {
        "id": "z21eQ6oJ0oFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "print(bs.find('img',{'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eE9Gye91kUS",
        "outputId": "db041e2d-a07a-40e7-8a42-e066dfeca44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "$15.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regular Expressions"
      ],
      "metadata": {
        "id": "NxUIJYPcpLCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "images = bs.find_all('img', {'src':re.compile('\\.\\./img\\/gifts/img.*\\.jpg')})\n",
        "images"
      ],
      "metadata": {
        "id": "p8Me2wq5aNMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in images:\n",
        "  print(image['src'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo-FzL_0pGUf",
        "outputId": "6234918a-5777-49d9-eaa3-40f2ff334d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../img/gifts/img1.jpg\n",
            "../img/gifts/img2.jpg\n",
            "../img/gifts/img3.jpg\n",
            "../img/gifts/img4.jpg\n",
            "../img/gifts/img6.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs.find_all(lambda tag: len(tag.attrs) == 2)"
      ],
      "metadata": {
        "id": "8BeF0D73phka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs.find_all(lambda tag: tag.get_text() ==\n",
        "    'Or maybe he\\'s only resting?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhRvOfTEqSvf",
        "outputId": "67503fb2-4d6c-49f3-8921-740310a992fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<span class=\"excitingNote\">Or maybe he's only resting?</span>]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Writing Web Crawlers\n"
      ],
      "metadata": {
        "id": "7cn0e6ukrELq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
        "bs = BeautifulSoup(html, 'html.parser')"
      ],
      "metadata": {
        "id": "J5nMSTzXrHyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in bs.find_all('a'):\n",
        "  if 'href' in link.attrs:\n",
        "    print(link.attrs['href'])"
      ],
      "metadata": {
        "id": "ao5lgmmTsf8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
        "  if 'href' in link.attrs:\n",
        "    print(link.attrs['href'])\n"
      ],
      "metadata": {
        "id": "luYCYD7isqix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import random"
      ],
      "metadata": {
        "id": "PRSaI9Fetdoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(datetime.datetime.now())"
      ],
      "metadata": {
        "id": "JwrejzE_zZ4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLinks(articleUrl):\n",
        "  html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "\n",
        "\n",
        "links = getLinks('/wiki/Kevin_Bacon')"
      ],
      "metadata": {
        "id": "0kihM_5LzcYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while len(links) > 0:\n",
        "  newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\n",
        "  print(newArticle)\n",
        "  link = getLinks(newArticle)"
      ],
      "metadata": {
        "id": "AYfLuyxkzzUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()"
      ],
      "metadata": {
        "id": "sHamfpT60LDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLinks(pageUrl):\n",
        "  global pages\n",
        "  html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
        "    if 'href' in link.attrs:\n",
        "      if link.attrs['href'] not in pages:\n",
        "        newPage = link.attrs['href']\n",
        "        print(newPage)\n",
        "        pages.add(newPage)\n",
        "        getLinks(newPage)\n",
        "      \n",
        "getLinks('')\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "5iAORdMd5cO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()\n",
        "def getLinks(pageUrl):\n",
        "    global pages\n",
        "    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    try:\n",
        "      print(bs.h1.get_text())\n",
        "      print(bs.find(id='mw-content-text').find_all('p')[0])\n",
        "      print(bs.find(id='ca-edit').find('span').find('a').attrs['href'])\n",
        "    except AttributeError:\n",
        "      print('This page is missing something! Continuing')\n",
        "\n",
        "    for link in bs.find_all('a', href=re.compile('^/(wiki)/')):\n",
        "      if 'href' in link.attrs:\n",
        "        if link.attrs['href'] not in pages:\n",
        "          #We have encountered a new page\n",
        "                newPage = link.attrs['href']\n",
        "                print('-'*20)\n",
        "                print(newPage)\n",
        "                pages.add(newPage)\n",
        "                getLinks(newPage)\n",
        "getLinks('')"
      ],
      "metadata": {
        "id": "pBzceReEBJHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "pages = set()\n",
        "random.seed(datetime.datetime.now())"
      ],
      "metadata": {
        "id": "3jeKFni5C9f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieves a list of all Internal links found on a page\n",
        "def getInternalLinks(bs, includeUrl):\n",
        "  includeUrl = f'{urlparse(includeUrl).scheme}://{urlparse(includeUrl).netloc}'\n",
        "  internalLinks = []\n",
        "  \n",
        "  #Finds all links that begin with a \"/\"\n",
        "  for link in bs.find_all('a', href=re.compile('^(/|.*' + includeUrl + ')')):\n",
        "    if link.attrs['href'] is not None:\n",
        "      if link.attrs['href'] not in internalLinks:\n",
        "        if(link.attrs['href'].startswith('/')):\n",
        "          internalLinks.append(includeUrl+link.attrs['href'])\n",
        "        else:\n",
        "          internalLinks.append(link.attrs['href'])\n",
        "  return internalLinks"
      ],
      "metadata": {
        "id": "3GOwxGVpHleF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieves a list of all external links found on a page\n",
        "\n",
        "def getExternalLinks(bs, excludeUrl):\n",
        "  externalLinks = []\n",
        "   #Finds all links that start with \"http\" that do\n",
        "   #not contain the current URL\n",
        "  for link in bs.find_all('a', href=re.compile('^(http|www)((?!' + excludeUrl + ').)*$')):\n",
        "    if link.attrs['href'] not in externalLinks:\n",
        "      externalLinks.append(link.attrs['href'])\n",
        "  return externalLinks"
      ],
      "metadata": {
        "id": "doqavsrdJDd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getRandomExternalLink(startingPage):\n",
        "    html = urlopen(startingPage)\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    externalLinks = getExternalLinks(bs, urlparse(startingPage).netloc)\n",
        "    if len(externalLinks) == 0:\n",
        "      print('No external links, looking around the site for one')\n",
        "      domain = f'{urlparse(startingPage).scheme}://{urlparse(startingPage).netloc}'\n",
        "      internalLinks = getInternalLinks(bs, domain)\n",
        "      return getRandomExternalLink(internalLinks[random.randint(0, len(internalLinks) -1)])\n",
        "    else:\n",
        "      return externalLinks[random.randint(0, len(externalLinks) -1)]"
      ],
      "metadata": {
        "id": "3atL14qhKLgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def followExternalOnly(startingSite):\n",
        "  externalLink = getRandomExternalLink(startingSite)\n",
        "  print(f'Random external link is: {externalLink}')\n",
        "  followExternalOnly(externalLink)\n",
        "  followExternalOnly('http://oreilly.com')"
      ],
      "metadata": {
        "id": "ck2ocSYIReY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "followExternalOnly('http://oreilly.com')"
      ],
      "metadata": {
        "id": "LT9dNtV_R3lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allExtLinks = set()\n",
        "allIntLinks = set()"
      ],
      "metadata": {
        "id": "q-EUCTPcSr--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAllExternalLinks(siteUrl):\n",
        "  html = urlopen(siteUrl)\n",
        "  domain = f'{urlparse(siteUrl).scheme}://{urlparse(siteUrl).netloc}'\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  internalLinks = getInternalLinks(bs, domain)\n",
        "  externalLinks = getExternalLinks(bs, domain)\n",
        "  \n",
        "  for link in externalLinks:\n",
        "    if link not in allExtLinks:\n",
        "      allExtLinks.add(link)\n",
        "      print(link)\n",
        "  for link in internalLinks:\n",
        "    if link not in allIntLinks:\n",
        "      allIntLinks.add(link)\n",
        "      getAllExternalLinks(link)"
      ],
      "metadata": {
        "id": "_ItZ9-EoTuVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allIntLinks.add('http://oreilly.com')\n",
        "getAllExternalLinks('http://oreilly.com')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6ZuKWNEU15C",
        "outputId": "53cb55d1-bdd4-4d1e-d4ab-c4974d84172a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.oreilly.com\n",
            "https://learning.oreilly.com/accounts/login-check/\n",
            "https://www.oreilly.com/online-learning/try-now.html\n",
            "https://www.oreilly.com/online-learning/teams.html\n",
            "https://www.oreilly.com/online-learning/government.html\n",
            "https://www.oreilly.com/online-learning/academic.html\n",
            "https://www.oreilly.com/online-learning/individuals.html\n",
            "https://www.oreilly.com/online-learning/features.html\n",
            "https://www.oreilly.com/online-learning/feature-certification.html\n",
            "https://www.oreilly.com/online-learning/intro-interactive-learning.html\n",
            "https://www.oreilly.com/online-learning/live-events.html\n",
            "https://www.oreilly.com/online-learning/feature-answers.html\n",
            "https://www.oreilly.com/radar/\n",
            "https://www.oreilly.com/content-marketing-solutions.html\n",
            "https://learning.oreilly.com/p/register/\n",
            "https://www.oreilly.com/online-learning/the-cost-of-doing-nothing.html\n",
            "https://learning.oreilly.com/search/?query=author%3A%22Arianne%20Dee%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=suggestion&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
            "https://learning.oreilly.com/search/?query=author%3A%22Bruno%20Gon%C3%A7alves%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
            "https://learning.oreilly.com/search/?query=author%3A%22Kelsey%20Hightower%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
            "https://learning.oreilly.com/search/?query=author%3A%22Sari%20Greene%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
            "https://learning.oreilly.com/search/?query=author%3A%22Neal%20Ford%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
            "https://learning.oreilly.com/search/?query=author%3A%22Ken%20Kousen%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
            "https://www.oreilly.com/online-learning/live-online-sessions.html\n",
            "https://www.oreilly.com/online-learning/enterprise.html\n",
            "https://www.oreilly.com/diversity/\n",
            "https://twitter.com/oreillymedia\n",
            "https://www.linkedin.com/company/oreilly-media\n",
            "https://www.youtube.com/user/OreillyMedia\n",
            "https://itunes.apple.com/us/app/safari-to-go/id881697395\n",
            "https://play.google.com/store/apps/details?id=com.safariflow.queue\n",
            "https://channelstore.roku.com/details/c8a2d0096693eb9455f6ac165003ee06/oreilly\n",
            "https://www.amazon.com/OReilly-Media-Inc/dp/B087YYHL5C/ref=sr_1_2?dchild=1&keywords=oreilly&qid=1604964116&s=mobile-apps&sr=1-2\n",
            "https://www.oreilly.com/privacy.html?donotsell=show\n",
            "https://www.oreilly.com/about/history.html\n",
            "https://www.oreilly.com/tim/\n",
            "https://www.oreilly.com/pub/pr/3351\n",
            "https://www.oreilly.com/pub/pr/3348\n",
            "https://www.oreilly.com/pub/pr/3345\n",
            "https://www.oreilly.com/press/\n",
            "https://www.oreilly.com/about/editorial_independence.html\n",
            "https://www.oreilly.com/about/resources.html\n",
            "https://www.oreilly.com/work-with-us/build-interactive-learning.html\n",
            "https://docs.google.com/document/d/1QQNhyXHavHY3H5Xgy6UgkLlOpgkhquvb9QRVmlSf3eU/copy\n",
            "https://docs.google.com/document/d/16B8ZmpEj-DULGNb8X2NjpQPUnZMWUn5kM1ZBaN7pSUs/copy\n",
            "https://www.oreilly.com/careers/\n",
            "https://www.oreilly.com/careers/#positions\n",
            "https://www.diversityjobs.com/oreilly?utm_campaign=TopEmployer&utm_medium=badge&utm_source=\n",
            "https://www.oreilly.com/partner/optimize-your-community-partnership.html\n",
            "https://app.oreilly.com/cs/user/register?x-redirect=http://www.oreilly.com/partner/signup.csp\n",
            "https://app.oreilly.com/cs/user/login?x-redirect=http://www.oreilly.com/partner/signup.csp\n",
            "https://signup.cj.com/member/brandedPublisherSignUp.do?air_refmerchantid=3812999\n",
            "http://www.cj.com/\n",
            "https://www.oreilly.com/oreilly/privacy.html\n",
            "https://www.oreilly.com/work-with-us.html\n",
            "https://www.oreilly.com/careers/index.csp#positions\n",
            "https://www.cio.com/article/3516012/women-in-tech-statistics-the-hard-truths-of-an-uphill-battle.html\n",
            "https://www.oreilly.com/content-marketing/available-products.html\n",
            "https://www.oreilly.com/content-sponsorship/testimonial-dataiku.html\n",
            "https://www.oreilly.com/content-sponsorship/testimonials.html\n",
            "https://www.oreilly.com/online-learning/support/getting-started.html\n",
            "https://www.oreilly.com/online-learning/support/features.html#certification\n",
            "https://www.oreilly.com/online-learning/support/content.html#missLiveEvent\n",
            "https://www.oreilly.com/online-learning/support/account.html#resetEmail\n",
            "https://www.oreilly.com/online-learning/support/content.html#offlineAccess\n",
            "https://www.oreilly.com/online-learning/support/account.html#cancelTrial\n",
            "https://www.oreilly.com/online-learning/support/apps.html\n",
            "https://www.oreilly.com/online-learning/support/news.html\n",
            "https://www.oreilly.com/online-learning/support/account.html\n",
            "https://www.oreilly.com/online-learning/support/content.html\n",
            "https://www.oreilly.com/online-learning/support/features.html\n",
            "https://www.oreilly.com/online-learning/support/accessibility.html\n",
            "https://www.oreilly.com/about/\n",
            "http://www.oreilly.com\n",
            "http://www.oreilly.com/about/sebastopol_directions.html\n",
            "http://www.oreilly.com.cn/\n",
            "http://www.oreilly.co.jp\n",
            "http://oreilly.com/oreilly/privacy.html\n",
            "https://www.oreilly.com/terms/\n",
            "https://learning.oreilly.com/membership-agreement/\n",
            "https://www.safaribooksonline.com/membership-agreement/\n",
            "https://www.safaribooksonline.com/terms/\n",
            "https://edpo.com/gdpr-data-request/\n",
            "https://www.oreilly.com/privacy-cookies.html\n",
            "https://www.oreilly.com/privacy-categories-disclosed-for-business-purpose.html\n",
            "https://www.oreilly.com/privacy-categories-disclosed-for-valuable-consideration.html\n",
            "https://www.privacyshield.gov\n",
            "https://www.privacyshield.gov/article?id=How-to-Submit-a-Complaint\n",
            "https://www.privacyshield.gov/article?id=ANNEX-I-introduction\n",
            "https://edpb.europa.eu/about-edpb/about-edpb/members_en\n",
            "https://www.edoeb.admin.ch/edoeb/en/home/the-fdpic/task.html\n",
            "https://ec.europa.eu/info/law/law-topic/data-protection/data-transfers-outside-eu/adequacy-protection-personal-data-non-eu-countries_en\n",
            "https://ec.europa.eu/info/law/law-topic/data-protection/data-transfers-outside-eu/model-contracts-transfer-personal-data-third-countries_en\n",
            "https://ico.org.uk/\n",
            "https://tools.google.com/dlpage/gaoptout/\n",
            "https://smetrics.oreilly.com/optout.html?optout=1&confirm_change=1\n",
            "https://www.oreilly.com/privacy-100721.html\n",
            "https://www.oreilly.com/privacy-072921.html\n",
            "https://www.oreilly.com/privacy-040721.html\n",
            "https://www.oreilly.com/privacy-010120.html\n",
            "https://www.oreilly.com/privacy-111219.html\n",
            "https://www.oreilly.com/privacy-081519.html\n",
            "https://www.oreilly.com/privacy-112118.html\n",
            "https://www.oreilly.com/privacy-101018.html\n",
            "https://www.oreilly.com/privacy-100418.html\n",
            "https://www.oreilly.com/privacy-052518.html\n",
            "https://www.oreilly.com/privacy-111016.html\n",
            "https://www.oreilly.com/privacy-120214.html\n",
            "https://www.oreilly.com/privacy-091415.html\n",
            "http://archive.oreilly.com/oreillyschool/privacy-policy/\n",
            "http://archive.oreilly.com/oreillyschool/privacy-policy-version2/\n",
            "https://cdn.oreillystatic.com/pdf/SafariPrivacyPolicy_v3.4_23October2017.pdf\n",
            "https://www.safaribooksonline.com/static/legal/SafariPrivacyPolicy_v3.3_13June2017.a4d9478408f5.pdf\n",
            "https://www.oreilly.com/terms/guidelines.html\n",
            "https://creativecommons.org/licenses/by-sa/3.0/\n",
            "https://www.copyright.gov/title17/92chap1.html#107\n",
            "https://www.oreilly.com/privacy.html\n",
            "http://radar.oreilly.com/2009/01/work-on-stuff-that-matters-fir.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Web Crawling Models"
      ],
      "metadata": {
        "id": "tAky39qxV7t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import datetime\n",
        "import random\n",
        "from urllib.parse import urlparse"
      ],
      "metadata": {
        "id": "Q2ODI4AeU2cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Content:\n",
        "  def __init__(self, url, title, body):\n",
        "    self.url = url\n",
        "    self.title = title\n",
        "    self.body = body\n",
        "\n",
        "def getPage(url):\n",
        "  req = requests.get(url)\n",
        "  return BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "def scrapeNYTimes(url):\n",
        "  bs = getPage(url)\n",
        "  title = bs.find('h1').text\n",
        "  lines = bs.find_all('p', {'class', 'story-content'})\n",
        "  body = '\\n'.join([line.text for line in lines])\n",
        "  return Content(url, title, body)\n",
        "\n",
        "def scrapeBrookings(url):\n",
        "  bs = getPage(url)\n",
        "  title = bs.find('h1').text\n",
        "  body = bs.find('div', {'class':'post-body'}).text\n",
        "  return Content(url, title, body)"
      ],
      "metadata": {
        "id": "VP8NA9DIVg0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'"
      ],
      "metadata": {
        "id": "6f8vsyPTkNFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = scrapeBrookings(url)"
      ],
      "metadata": {
        "id": "8fJeTatSkZF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Title: {content.title}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jgaohEWkdME",
        "outputId": "faaa0181-0a52-4cd8-ecef-0d81b9f644c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Delivering inclusive urban access: 3 uncomfortable truths\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'URL: {content.url}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF3IsBNhk8CU",
        "outputId": "787fc7e6-8903-4767-8572-d017b4dc3600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(content.body)"
      ],
      "metadata": {
        "id": "kPgDQThnlFgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url2 = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'"
      ],
      "metadata": {
        "id": "4WpK6MEzlGXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content2 = scrapeNYTimes(url2)"
      ],
      "metadata": {
        "id": "TmFHF4bxlL6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Title: {}'.format(content2.title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCsVqq2rlTW5",
        "outputId": "07e0280a-d2ce-47f2-d9bd-7f5e912fad8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: The Men Who Want to Live Forever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('URL: {}\\n'.format(content2.url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OV8-J84lVgq",
        "outputId": "c5fe0f94-e6f5-4a76-c44f-add6300534ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(content2.body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rugP2SfKlWye",
        "outputId": "d320a8d4-0d8a-4240-8166-cabc97bcb4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Content:\n",
        "  def __init__(self, url, title, body):\n",
        "    self.url = url\n",
        "    self.title = title\n",
        "    self.body = body\n",
        "\n",
        "  def print(self):\n",
        "      \"\"\"\n",
        "      Flexible printing function controls output\n",
        "      \"\"\"\n",
        "      print(\"URL: {}\".format(self.url))\n",
        "      print(\"TITLE: {}\".format(self.title))\n",
        "      print(\"BODY:\\n{}\".format(self.body))"
      ],
      "metadata": {
        "id": "4GCX3ZOnlX6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "  def __init__(self, name, url, titleTag, bodyTag):\n",
        "    self.name = name\n",
        "    self.url = url\n",
        "    self.titleTag = titleTag\n",
        "    self.bodyTag = bodyTag"
      ],
      "metadata": {
        "id": "XpnkKB8Kqn3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crawler:\n",
        "  def getPage(self, url):\n",
        "    try:\n",
        "      req = requests.get(url)\n",
        "    except requests.exceptions.RequestException:\n",
        "      return None\n",
        "    return BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "  def safeGet(self, pageObj, selector):\n",
        "    selectedElems = pageObj.select(selector)\n",
        "    if selectedElems is not None and len(selectedElems) > 0:\n",
        "      return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
        "    return ''\n",
        "\n",
        "  def parse(self, site, url):\n",
        "    bs = self.getPage(url)\n",
        "    if bs is not None:\n",
        "      title = self.safeGet(bs, site.titleTag)\n",
        "      body = self.safeGet(bs, site.bodyTag)\n",
        "      if title != '' and body != '':\n",
        "        content = Content(url, title, body)\n",
        "        content.print()"
      ],
      "metadata": {
        "id": "2JTL9OTJreHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawler = Crawler()"
      ],
      "metadata": {
        "id": "LQQSWqjruhg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siteData = [\n",
        "    ['O\\'Reilly Media', 'http://oreilly.com',\n",
        "    'h1', 'section#product-description'],\n",
        "    ['Reuters', 'http://reuters.com', 'h1',\n",
        "    'div.StandardArticleBody_body_1gnLA'],\n",
        "    ['Brookings', 'http://www.brookings.edu',\n",
        "    'h1', 'div.post-body'],\n",
        "    ['New York Times', 'http://nytimes.com',\n",
        "    'h1', 'p.story-content']]"
      ],
      "metadata": {
        "id": "lUjh4jCnuoqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "websites = []"
      ],
      "metadata": {
        "id": "isS7oOn4uu3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in siteData:\n",
        "  websites.append(Website(row[0], row[1], row[2], row[3]))"
      ],
      "metadata": {
        "id": "mLpTT6c7u0bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawler.parse(websites[0], 'http://shop.oreilly.com/product/'\\\n",
        "    '0636920028154.do')\n",
        "crawler.parse(websites[1], 'http://www.reuters.com/article/'\\\n",
        "    'us-usa-epa-pruitt-idUSKBN19W2D0')\n",
        "crawler.parse(websites[2], 'https://www.brookings.edu/blog/'\\\n",
        "    'techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
        "crawler.parse(websites[3], 'https://www.nytimes.com/2018/01/'\\\n",
        "    '28/business/energy-environment/oil-boom.html')"
      ],
      "metadata": {
        "id": "V0ymkfOku-px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Content:\n",
        "  def __init__(self, topic, url, title, body):\n",
        "      self.topic = topic\n",
        "      self.title = title\n",
        "      self.body = body\n",
        "      self.url = url\n",
        "  def print(self):\n",
        "      \"\"\"\n",
        "      Flexible printing function controls output\n",
        "      \"\"\"\n",
        "      print(\"New article found for topic: {}\".format(self.topic))\n",
        "      print(\"TITLE: {}\".format(self.title))\n",
        "      print(\"BODY:\\n{}\".format(self.body))\n",
        "      print(\"URL: {}\".format(self.url))"
      ],
      "metadata": {
        "id": "gB58_FikvFXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    \"\"\"Contains information about website structure\"\"\"\n",
        "    def __init__(self, name, url, searchUrl, resultListing,\n",
        "        resultUrl, absoluteUrl, titleTag, bodyTag):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.searchUrl = searchUrl\n",
        "        self.resultListing = resultListing\n",
        "        self.resultUrl = resultUrl\n",
        "        self.absoluteUrl=absoluteUrl\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag"
      ],
      "metadata": {
        "id": "WjBoepmC3Z76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crawler:\n",
        "    def getPage(self, url):\n",
        "        try:\n",
        "            req = requests.get(url)\n",
        "        except requests.exceptions.RequestException:\n",
        "            return None\n",
        "        return BeautifulSoup(req.text, 'html.parser')\n",
        "    def safeGet(self, pageObj, selector):\n",
        "        childObj = pageObj.select(selector)\n",
        "        if childObj is not None and len(childObj) > 0:\n",
        "            return childObj[0].get_text()\n",
        "        return \"\"\n",
        "    def search(self, topic, site):\n",
        "        \"\"\"\n",
        "        Searches a given website for a given topic and records all pages found\n",
        "        \"\"\"\n",
        "        bs = self.getPage(site.searchUrl + topic)\n",
        "        searchResults = bs.select(site.resultListing)\n",
        "        for result in searchResults:\n",
        "            url = result.select(site.resultUrl)[0].attrs[\"href\"]\n",
        "            # Check to see whether it's a relative or an absolute URL\n",
        "            if(site.absoluteUrl):\n",
        "                bs = self.getPage(url)\n",
        "            else:\n",
        "                bs = self.getPage(site.url + url)\n",
        "            if bs is None:\n",
        "                print(\"Something was wrong with that page or URL. Skipping!\")\n",
        "                return\n",
        "            title = self.safeGet(bs, site.titleTag)\n",
        "            body = self.safeGet(bs, site.bodyTag)\n",
        "            if title != '' and body != '':\n",
        "                content = Content(topic, title, body, url)\n",
        "                content.print()\n",
        "crawler = Crawler()\n",
        "siteData = [['O\\'Reilly Media', 'http://oreilly.com',\n",
        "        'https://ssearch.oreilly.com/?q=','article.product-result',\n",
        "        'p.title a', True, 'h1', 'section#product-description'],\n",
        "    ['Reuters', 'http://reuters.com',\n",
        "        'http://www.reuters.com/search/news?blob=',\n",
        "        'div.search-result-content','h3.search-result-title a',\n",
        "        False, 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
        "    ['Brookings', 'http://www.brookings.edu',\n",
        "        'https://www.brookings.edu/search/?s=',\n",
        "        'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']]\n",
        "\n",
        "sites = []\n",
        "for row in siteData:\n",
        "    sites.append(Website(row[0], row[1], row[2],\n",
        "                         row[3], row[4], row[5], row[6], row[7]))\n",
        "topics = ['python', 'data science']\n",
        "for topic in topics:\n",
        "    print(\"GETTING INFO ABOUT: \" + topic)\n",
        "    for targetSite in sites:\n",
        "        crawler.search(topic, targetSite)"
      ],
      "metadata": {
        "id": "ownxJLfs4cx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    def __init__(self, name, url, targetPattern, absoluteUrl,\n",
        "        titleTag, bodyTag):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.targetPattern = targetPattern\n",
        "        self.absoluteUrl=absoluteUrl\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag\n",
        "       \n",
        "class Content:\n",
        "    def __init__(self, url, title, body):\n",
        "        self.url = url\n",
        "        self.title = title\n",
        "        self.body = body\n",
        "    def print(self):\n",
        "        print(\"URL: {}\".format(self.url))\n",
        "        print(\"TITLE: {}\".format(self.title))\n",
        "        print(\"BODY:\\n{}\".format(self.body))"
      ],
      "metadata": {
        "id": "5L6CehAg4zIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crawler:\n",
        "    def __init__(self, site):\n",
        "        self.site = site\n",
        "        self.visited = []\n",
        "       \n",
        "    def getPage(self, url):\n",
        "        try:\n",
        "            req = requests.get(url)\n",
        "        except requests.exceptions.RequestException:\n",
        "            return None       \n",
        "        return BeautifulSoup(req.text, 'html.parser')\n",
        "    \n",
        "    def safeGet(self, pageObj, selector):\n",
        "        selectedElems = pageObj.select(selector)\n",
        "        if selectedElems is not None and len(selectedElems) > 0:\n",
        "            return '\\n'.join([elem.get_text() for\n",
        "                elem in selectedElems])\n",
        "        return ''\n",
        "   \n",
        "    def parse(self, url):\n",
        "        bs = self.getPage(url)\n",
        "        if bs is not None:\n",
        "            title = self.safeGet(bs, self.site.titleTag)\n",
        "            body = self.safeGet(bs, self.site.bodyTag)\n",
        "            if title != '' and body != '':\n",
        "                content = Content(url, title, body)\n",
        "                content.print()\n",
        "    def crawl(self):\n",
        "        \"\"\"\n",
        "        Get pages from website home page\n",
        "        \"\"\"\n",
        "        bs = self.getPage(self.site.url)\n",
        "        targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
        "        for targetPage in targetPages:\n",
        "            targetPage = targetPage.attrs['href']\n",
        "            if targetPage not in self.visited:\n",
        "                self.visited.append(targetPage)\n",
        "                if not self.site.absoluteUrl:\n",
        "                    targetPage = '{}{}'.format(self.site.url, targetPage)\n",
        "                self.parse(targetPage)\n",
        "\n",
        "reuters = Website('Reuters', 'https://www.reuters.com', '^(/article/)', False,\n",
        "    'h1', 'div.StandardArticleBody_body_1gnLA')\n",
        "crawler = Crawler(reuters)\n",
        "crawler.crawl()"
      ],
      "metadata": {
        "id": "hS6Xa3kM__lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    \"\"\"Common base class for all articles/pages\"\"\"\n",
        "    def __init__(self, type, name, url, searchUrl, resultListing,\n",
        "        resultUrl, absoluteUrl, titleTag, bodyTag):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag\n",
        "        self.pageType = pageType"
      ],
      "metadata": {
        "id": "luwOfxmRAY40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Storing Data"
      ],
      "metadata": {
        "id": "9GF2d5khEZtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import urlretrieve\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import datetime\n",
        "import random\n",
        "from urllib.parse import urlparse"
      ],
      "metadata": {
        "id": "VZs6oD-MEdKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('http://www.pythonscraping.com')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "imageLocation = bs.find('img', {'class': 'pagelayer-img pagelayer-wp-title-img'})['src']\n",
        "urlretrieve (imageLocation, 'logo01.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcWjid7E_WQ1",
        "outputId": "bf771108-5add-4264-e335-37afaafc33d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('logo01.png', <http.client.HTTPMessage at 0x7fdd239dd110>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imageLocation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFKJvJ99ADqp",
        "outputId": "3fbe1e18-2df6-46b4-d445-2e0f93bc78cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://pythonscraping.com/wp-content/uploads/2021/08/logo01.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "downloadDirectory = 'downloaded'\n",
        "baseUrl = 'http://pythonscraping.com'\n",
        "\n",
        "def getAbsoluteURL(baseUrl, source):\n",
        "    if source.startswith('http://www.'):\n",
        "        url = 'http://{}'.format(source[11:])\n",
        "    elif source.startswith('http://'):\n",
        "        url = source\n",
        "    elif source.startswith('www.'):\n",
        "        url = source[4:]\n",
        "        url = 'http://{}'.format(source)\n",
        "    else:\n",
        "        url = '{}/{}'.format(baseUrl, source)\n",
        "    if baseUrl not in url:\n",
        "        return None\n",
        "    return url\n",
        "\n",
        "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
        "    path = absoluteUrl.replace('www.', '')\n",
        "    path = path.replace(baseUrl, '')\n",
        "    path = downloadDirectory+path\n",
        "    directory = os.path.dirname(path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    return path\n",
        "\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "downloadList = bs.findAll(src=True)\n",
        "for download in downloadList:\n",
        "    fileUrl = getAbsoluteURL(baseUrl, download['src'])\n",
        "    if fileUrl is not None:\n",
        "        print(fileUrl)\n",
        "\n",
        "\n",
        "urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
      ],
      "metadata": {
        "id": "5BnsI-DpAdQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "csvFile = open('test.csv', 'w+')\n",
        "\n",
        "try:\n",
        "    writer = csv.writer(csvFile)\n",
        "    writer.writerow(('number', 'number plus 2', 'number times 2'))\n",
        "    for i in range(10):\n",
        "        writer.writerow( (i, i+2, i*2))\n",
        "finally:\n",
        "    csvFile.close()"
      ],
      "metadata": {
        "id": "1yxKoCkhBDrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "table = bs.findAll('table',{'class':'wikitable'})[0]\n",
        "table"
      ],
      "metadata": {
        "id": "T-yOmq1hDUpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = table.findAll('tr')\n",
        "rows"
      ],
      "metadata": {
        "id": "ASI--X22FlHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csvFile = open('editors.csv', 'wt+')\n",
        "writer = csv.writer(csvFile)\n",
        "\n",
        "try:\n",
        "    for row in rows:\n",
        "        csvRow = []\n",
        "        for cell in row.findAll(['td', 'th']):\n",
        "            csvRow.append(cell.get_text())\n",
        "            writer.writerow(csvRow)\n",
        "finally:\n",
        "    csvFile.close()"
      ],
      "metadata": {
        "id": "Td-4a4I7FpRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtQ-aQFRH180"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}